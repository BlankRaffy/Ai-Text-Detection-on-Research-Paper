<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE collection SYSTEM "BioC.dtd">
<collection><source>PMC</source><date>20230508</date><key>pmc.key</key><document><id>35360</id><infon key="license">NO-CC CODE</infon><passage><infon key="article-id_doi">10.1186/1471-2105-2-3</infon><infon key="article-id_pmc">35360</infon><infon key="article-id_pmid">11483157</infon><infon key="article-id_publisher-id">1471-2105-2-3</infon><infon key="fpage">3</infon><infon key="journal-title">BMC Bioinformatics</infon><infon key="lpage">3</infon><infon key="name_0">surname:Cai;given-names:Yu-Dong</infon><infon key="name_1">surname:Liu;given-names:Xiao-Jun</infon><infon key="name_2">surname:Xu;given-names:Xue-biao</infon><infon key="name_3">surname:Zhou;given-names:Guo-Ping</infon><infon key="section_type">TITLE</infon><infon key="type">front</infon><infon key="volume">2</infon><infon key="year">2001</infon><offset>0</offset><text>Support Vector Machines for predicting protein structural class</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>64</offset><text>Background</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>75</offset><text>We apply a new machine learning method, the so-called Support Vector Machine method, to predict the protein structural class. Support Vector Machine method is performed based on the database derived from SCOP, in which protein domains are classified based on known structures and the evolutionary relationships and the principles that govern their 3-D structure.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>438</offset><text>Results</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>446</offset><text>High rates of both self-consistency and jackknife tests are obtained. The good results indicate that the structural class of a protein is considerably correlated with its amino acid composition.</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract_title_1</infon><offset>641</offset><text>Conclusions</text></passage><passage><infon key="section_type">ABSTRACT</infon><infon key="type">abstract</infon><offset>653</offset><text>It is expected that the Support Vector Machine method and the elegant component-coupled method, also named as the covariant discrimination algorithm, if complemented with each other, can provide a powerful computational tool for predicting the structural classes of proteins.</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">title_1</infon><offset>929</offset><text>Introduction</text></passage><passage><infon key="section_type">INTRO</infon><infon key="type">paragraph</infon><offset>942</offset><text>The observed results by Muskal and Kim suggested that the structural class of a protein might basically depend on its amino acid composition. Many efforts have been made to predict the structural class of a protein based on its amino acid composition. The physical mechanism about this kind of correlation has been discussed by Bahar et al. and Chou. For a systematic description in this area, see a comprehensive review by Chou and Zhang and an updated review. In this paper, we try to apply Vapnik's Support Vector Machine to approach this problem. In this work. Support Vector Machine was performed based on the data sets constructed by Zhou based on SCOP. In ref.19 the reason why these data sets are more reasonable has also been addressed. As a result, high rates of self-consistency and jackknife test were obtained. This has further confirmed that the structural class of a protein is considerably correlated with its amino acid composition.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_1</infon><offset>1892</offset><text>Results and Discussion</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>1915</offset><text>Success rate of self-consistency of SVMs</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>1956</offset><text>In this research, the examination for the self-consistency of the SVM method was tested. The following two data sets from Zhou are used. One consists of 277 domains, of which 70 all-α domains, 61 all-β domains, 81 α/β domains, and 65 α+β domains. The other data set consists of 498 domains, of which 107 are all-α domains, 126 all-β,136 α/β domains, and 129 α+β domains. All the rates of correct prediction for the four structural classes of both datasets reach 100%. These rates are &quot;training&quot; accuracy, indicating that after being trained, the SVM model has grasped the complicated relationship between the amino acid composition and protein structure.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>2653</offset><text>Success rate of jackknife test of SVMs</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>2692</offset><text>We use jackknife test for cross-validation. The cross-validation by jackknifing is thought the most objective and rigorous way in comparison with sub-sampling test or independent dataset test. During the process of jackknife analysis, the datasets are actually open, and a protein will in turn move from each to the other. As a result, the overall rate of correct prediction for the four structural classes of 277 domains (the 1 st set) was 220/277 = 79.4%; while the rates of correct prediction for the four structural classes of 498 domains (the 2nd set) was 464/498 = 93.2%.</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">title_2</infon><offset>3270</offset><text>Comparison to neural network method and elegant component-coupled algorithm</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>3346</offset><text>Zhou applied the elegant component-coupled algorithm developed by Chou et al. to protein structure class prediction. Later Cai and Zhou applied neural network method to the same problem. The comparison of their results to SVM method is given in Table 1 (for self-consistency test) and Table 2 (for jackknife test).</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>3661</offset><text>Results of Self-Consistency Test</text></passage><passage><infon key="file">T1.xml</infon><infon key="id">T1</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Dataset&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Algorithm&lt;/td&gt;&lt;td align=&quot;left&quot; colspan=&quot;4&quot;&gt;Rate of correct prediction for each class&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Overall&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td/&gt;&lt;td colspan=&quot;4&quot;&gt;&lt;hr/&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Rate of&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td/&gt;&lt;td/&gt;&lt;td/&gt;&lt;td/&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;Correct&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;all-α&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;all-β&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;α/β&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;α+β.&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Prediction&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;277 domains&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;component coupled&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.7%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;93.4%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.1%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;92.3%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;94.2%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;neural network&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;98.6%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;93.4%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;96.3%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;84.6%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;93.5%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;SVM&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;498 domains&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;component coupled&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.8%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.2%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;94.9%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.4%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.8%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;neural network&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;98.4%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;96.3%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;84.5%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;94.6%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;SVM&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;100%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>3694</offset><text>Dataset	Algorithm	Rate of correct prediction for each class	Overall	 				Rate of	 							Correct	 			all-α	all-β	α/β	α+β.	Prediction	 	277 domains	component coupled	95.7%	93.4%	95.1%	92.3%	94.2%	 		neural network	98.6%	93.4%	96.3%	84.6%	93.5%	 		SVM	100%	100%	100%	100%	100%	 	498 domains	component coupled	95.8%	95.2%	94.9%	95.4%	95.8%	 		neural network	100%	98.4%	96.3%	84.5%	94.6%	 		SVM	100%	100%	100%	100%	100%	 	</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table_caption</infon><offset>4134</offset><text>Results of Jackknife Test</text></passage><passage><infon key="file">T2.xml</infon><infon key="id">T2</infon><infon key="section_type">TABLE</infon><infon key="type">table</infon><infon key="xml">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;
&lt;table frame=&quot;hsides&quot; rules=&quot;groups&quot;&gt;&lt;thead&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;Dataset&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Algorithm&lt;/td&gt;&lt;td align=&quot;left&quot; colspan=&quot;4&quot;&gt;Rate of correct prediction for each class&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Overall&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td/&gt;&lt;td colspan=&quot;4&quot;&gt;&lt;hr/&gt;&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Rate of&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td/&gt;&lt;td/&gt;&lt;td/&gt;&lt;td/&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;Correct&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;all-α&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;all-β&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;α/β&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;α+β&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;Prediction&lt;/td&gt;&lt;/tr&gt;&lt;/thead&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;277 domains&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;component coupled&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;84.3%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;82.0%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;81.5%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;67.7%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;79.1%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;neural network&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;68.6%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;85.2%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;86.4%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;56.9%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;74.7%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;SVM&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;74.3%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;82.0%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;87.7%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;72.3%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;79.4%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;498 domains&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;component coupled&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;93.5%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;88.9%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;90.4%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;84.5%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;89.2%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;neural network&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;86.0%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;96.0%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;88.2%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;86.0%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;89.2%&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td align=&quot;left&quot;&gt;SVM&lt;/td&gt;&lt;td/&gt;&lt;td align=&quot;left&quot;&gt;88.8%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;95.2%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;96.3%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;91.5%&lt;/td&gt;&lt;td align=&quot;left&quot;&gt;93.2%&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;
</infon><offset>4160</offset><text>Dataset	Algorithm	Rate of correct prediction for each class	Overall	 				Rate of	 							Correct	 			all-α	all-β	α/β	α+β	Prediction	 	277 domains	component coupled	84.3%	82.0%	81.5%	67.7%	79.1%	 		neural network	68.6%	85.2%	86.4%	56.9%	74.7%	 		SVM	74.3%	82.0%	87.7%	72.3%	79.4%	 	498 domains	component coupled	93.5%	88.9%	90.4%	84.5%	89.2%	 		neural network	86.0%	96.0%	88.2%	86.0%	89.2%	 	SVM		88.8%	95.2%	96.3%	91.5%	93.2%	 	</text></passage><passage><infon key="section_type">RESULTS</infon><infon key="type">paragraph</infon><offset>4610</offset><text>The comparison should be focused on the jackknife rates (Table 2) because it represents the rate obtained by following a more objective test procedure. From Table 2 we can see that the rates of both the SVM and the component-coupled algorithm are higher than those of neural network. Although the rates obtained here by SVM are slightly higher than those by the component-coupled algorithm, it does not mean the predicted results by SVM are always better than those by the component-coupled algorithm. For some cases, the results obtained by the latter might be better than those by the former. Accordingly, it is expected, the SVM method and the component-coupled algorithm, if complemented with each other, will provide a powerful tool for predicting protein structural class.</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">title_1</infon><offset>5389</offset><text>Conclusion</text></passage><passage><infon key="section_type">CONCL</infon><infon key="type">paragraph</infon><offset>5400</offset><text>The current study has further supported, from the approach of SVMs, the conclusion drawn by Chou and his co-workers and Zhou that if the coupling effect among different amino acid components can be properly taken into account, the prediction quality of protein structural classes can be significantly improved.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_1</infon><offset>5711</offset><text>Materials and Methods</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>5733</offset><text>Support Vector Machine (SVM)</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>5762</offset><text>Support Vector Machine (SVM) is one kind of learning machine based on statistical learning theory. The basic idea of applying SVM to pattern classification can be stated briefly as follows. First, map the input vectors into one feature space (possible with a higher dimension), either linearly or non-linearly, which is relevant with the selection of the kernel function. Then, within the feature space from the first step, seek an optimized linear division, i.e. construct a hyperplane which separates two classes(this can be extended to multi-class). SVM training always seeks a global optimized solution and avoids over-fitting, so it has the ability to deal with a large number of features. A complete description to the theory of SVMs for pattern recognition is in Vapnik's book </text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6547</offset><text>SVMs have been used in a wide range of problems including drug design, image recognition and text classification, microarray gene expression data analysis, and protein fold recognition.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>6733</offset><text>In this paper, we apply Vapnik's Support Vector Machine for the structural classes of proteins. We download the SVMIight, which is an implementation (in C Language) of SVM for the problem of pattern recognition. The optimization algorithm used in SVMIight can be found in. The code has been used in text classification, image recognition, microarray gene expression data analysis and protein fold recognition.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7143</offset><text>Suppose we are given a set of samples, i.e, a series of input vectors</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7213</offset><text>with corresponding labels .</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7241</offset><text>Where -1 and +1 are used to stand respectively for the two classes. The goal here is to construct one binary classifier or derive one decision function from the available samples, which has small probability of misclassifying a future sample. Both the basic linear separable case and the most useful linear non-separable case for most real life problems are considered here:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>7616</offset><text>The linear separable case</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7642</offset><text>In this case, there exists a separating hyper plane whose function is , which implies:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>7729</offset><text>By minimizing  subject to this constraint, the SVM approach tries to find a unique separating hyperplane. Here  is the Euclidean norm of  which maximizes the distance between the hyper plane, i.e. Optimal Separating Hyperplane or OSH, and the nearest data points of each class. The classifier is called the largest margin classifier. By introducing Lagrange multipliers , the SVM training procedure amounts to solving a convex QP problem. The solution is a unique globally optimized result can be shown having the following expansion:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8264</offset><text>Only if the corresponding  &gt; 0, these  are called Support Vectors. When a SVM is trained, the decision function can be written as:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8395</offset><text>Where sgn() in the above formula is the given sign function.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>8456</offset><text>The linear non-separable case</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8486</offset><text>(i) &quot;soft margin&quot; technique.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8515</offset><text>In order to allow for training errors, ref.31 introduced slack variables:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8589</offset><text>ξi &gt; 0, i = 1, ..., N</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8612</offset><text>And relaxed separation constraint is given as:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8659</offset><text>And the OSH can be found by minimizing</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8698</offset><text>Where C is a regularization parameter used to decide a trade- off between the training error and the margin.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8807</offset><text>(ii) &quot;kernel substitution&quot; technique</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>8844</offset><text>SVM performs a nonlinear mapping of the input vector  from the input space  into a higher dimensional Hilbert space, where the mapping is determined by the kernel function. Then like in case (i), it finds the OSH in the space H corresponding to a non-linear boundary in the input space. Two typical kernel functions are listed below:</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9178</offset><text>And the form of the decision function is</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9219</offset><text>For a given data set, only the kernel function and the regularity parameter C must be selected to specify one SVM.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">title_2</infon><offset>9334</offset><text>The Training and Prediction of Protein Structural Class</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9390</offset><text>According to the SCOP database, the protein domains generally fall into one of the following four classes: (1) all-α, (2) all-β, (3) α/β, (4) α+β.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>9558</offset><text>According to its amino acid composition, a protein domain can be represented by a point or a vector in a 20-D space. However, of the 20 amino acid composition components, only 19 are independent due to the normalisation condition. Accordingly, strictly speaking, if based on amino acid composition, a protein should be represented by a point or a vector in a 19-D space rather than 20-D space as defined in a conventional manner. Furthermore, according to Chou's invariance theorem, the final predicted result will remain the same regardless of which one of the 20 components is left out for forming the 19-D space. It is extremely important to realize this, particularly when the calculations involve a covariance matrix such as in the case of refs.11-14. For the current study, the amino acid composition was used as the input of the SVM.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10399</offset><text>The SVM method applies to two-class problems. In this paper, for the four-class problems, we use a simple and effective method: &quot;one-against-others&quot; method to transfer it into two-class problems.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10595</offset><text>The computations were carried out on a Silicon Graphics IRIS Indigo work station (Elan 4000).</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>10689</offset><text>In this research, for the SVM, the width of the Gaussian RBFs is selected as that which minimized an estimate of the VC-dimension. The parameter C that controls the error-margin tradeoff is set at 100. After being trained, the hyperplane output by the SVM was obtained. This indicates that the trained model, i.e. hyperplane output which is including the important information, has the function to identify protein structural classes.</text></passage><passage><infon key="section_type">METHODS</infon><infon key="type">paragraph</infon><offset>11124</offset><text>We first test the self-consistency of the method, latterly is to test the method by cross-validation (jackknife test). As a result, the rates of both self-consistency and cross-validation were quite high.</text></passage><passage><infon key="fpage">713</infon><infon key="lpage">727</infon><infon key="name_0">surname:Muskal;given-names:SM</infon><infon key="name_1">surname:Kim;given-names:SH</infon><infon key="pub-id_pmid">1602478</infon><infon key="section_type">REF</infon><infon key="source">J Mol Biol</infon><infon key="type">ref</infon><infon key="volume">225</infon><infon key="year">1992</infon><offset>11329</offset><text>Predicting protein secondary structure content: A tandem neural network approach</text></passage><passage><infon key="name_0">surname:Chou;given-names:PY</infon><infon key="section_type">REF</infon><infon key="source">in Abstracts of Papers, Part I, Second Chemical Congress of the North AmericanContinent Las Vegas, Nevada,</infon><infon key="type">ref</infon><infon key="year">1980</infon><offset>11410</offset><text>Amino Acid composition of four classes of protein,</text></passage><passage><infon key="fpage">549</infon><infon key="lpage">586</infon><infon key="name_0">surname:Chou;given-names:PY</infon><infon key="section_type">REF</infon><infon key="source">Prediction of Protein Structure and the Principles ofProtein Conformation, ed. Fasman, G.D., Plenum Press: New York.</infon><infon key="type">ref</infon><infon key="year">1989</infon><offset>11461</offset><text>Prediction of protein structural classes from amino acid composition</text></passage><passage><infon key="fpage">152</infon><infon key="lpage">162</infon><infon key="name_0">surname:Nakashima;given-names:H</infon><infon key="name_1">surname:Nishikawa;given-names:K</infon><infon key="name_2">surname:Ooi;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">J Biochem</infon><infon key="type">ref</infon><infon key="volume">99</infon><infon key="year">1986</infon><offset>11530</offset><text>The folding type of a protein is relevant to the amino acid composition</text></passage><passage><infon key="fpage">1659</infon><infon key="lpage">1672</infon><infon key="name_0">surname:Klein;given-names:P</infon><infon key="name_1">surname:Delisi;given-names:C</infon><infon key="pub-id_pmid">3768479</infon><infon key="section_type">REF</infon><infon key="source">Biopolymers</infon><infon key="type">ref</infon><infon key="volume">25</infon><infon key="year">1986</infon><offset>11602</offset><text>Prediction of protein structural class from amino acid sequence</text></passage><passage><infon key="fpage">401</infon><infon key="lpage">408</infon><infon key="name_0">surname:Zhang;given-names:CT</infon><infon key="name_1">surname:Chou;given-names:KC</infon><infon key="pub-id_pmid">1304347</infon><infon key="section_type">REF</infon><infon key="source">Protein Science</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">1992</infon><offset>11666</offset><text>An optimization approach to predicting protein structural class from amino acid composition</text></passage><passage><infon key="fpage">79</infon><infon key="lpage">91</infon><infon key="name_0">surname:Dubchak;given-names:I</infon><infon key="name_1">surname:Holbrook;given-names:SR</infon><infon key="name_2">surname:Kim;given-names:SH</infon><infon key="section_type">REF</infon><infon key="source">Proteins: Structure, Function and Genetics</infon><infon key="type">ref</infon><infon key="volume">16</infon><infon key="year">1993</infon><offset>11758</offset><text>Predicting protein secondary structure content: A tandem neural network approach</text></passage><passage><infon key="fpage">1171</infon><infon key="lpage">1182</infon><infon key="name_0">surname:Metfessel;given-names:BA</infon><infon key="name_1">surname:Saurugger;given-names:PN</infon><infon key="name_2">surname:Connelly;given-names:DP</infon><infon key="name_3">surname:Rich;given-names:ST</infon><infon key="pub-id_pmid">8358300</infon><infon key="section_type">REF</infon><infon key="source">Protein Science</infon><infon key="type">ref</infon><infon key="volume">2</infon><infon key="year">1993</infon><offset>11839</offset><text>Cross-validation of protein structural class prediction using statistical clustering and neural networks</text></passage><passage><infon key="fpage">55</infon><infon key="lpage">72</infon><infon key="name_0">surname:Rost;given-names:B</infon><infon key="name_1">surname:Sander;given-names:C</infon><infon key="section_type">REF</infon><infon key="source">Protein: Struc Func, and Genetics</infon><infon key="type">ref</infon><infon key="volume">19</infon><infon key="year">1994</infon><offset>11944</offset><text>Combining evolutionary information and neural networks to predict protein secondary structure</text></passage><passage><infon key="fpage">275</infon><infon key="lpage">285</infon><infon key="name_0">surname:Chandonia;given-names:JM</infon><infon key="name_1">surname:Karplus;given-names:M</infon><infon key="pub-id_pmid">7757016</infon><infon key="section_type">REF</infon><infon key="source">Protein Science</infon><infon key="type">ref</infon><infon key="volume">4</infon><infon key="year">1995</infon><offset>12038</offset><text>Neural networks for secondary structure and structural class prediction</text></passage><passage><infon key="fpage">319</infon><infon key="lpage">344</infon><infon key="name_0">surname:Chou;given-names:KC</infon><infon key="section_type">REF</infon><infon key="source">Proteins: Structure, Function and Genetics,</infon><infon key="type">ref</infon><infon key="volume">21</infon><infon key="year">1995</infon><offset>12110</offset><text>A novel approach to predicting protein structural classes in a (20-1)-D amino acid composition space</text></passage><passage><infon key="fpage">523</infon><infon key="lpage">538</infon><infon key="name_0">surname:Chou;given-names:KC</infon><infon key="name_1">surname:Maggiora;given-names:GM</infon><infon key="pub-id_doi">10.1093/protein/11.7.523</infon><infon key="section_type">REF</infon><infon key="source">Proteins Engineering,</infon><infon key="type">ref</infon><infon key="volume">11</infon><infon key="year">1998</infon><offset>12211</offset><text>Domain structural class prediction</text></passage><passage><infon key="fpage">97</infon><infon key="lpage">103</infon><infon key="name_0">surname:Chou;given-names:KC</infon><infon key="name_1">surname:Liu;given-names:W</infon><infon key="name_2">surname:Maggiora;given-names:GM</infon><infon key="name_3">surname:Zhang;given-names:CT</infon><infon key="pub-id_doi">10.1002/(SICI)1097-0134(19980401)31:1&lt;97::AID-PROT8&gt;3.0.CO;2-E</infon><infon key="section_type">REF</infon><infon key="source">Proteins: Structure, Function and Genetics</infon><infon key="type">ref</infon><infon key="volume">31</infon><infon key="year">1998</infon><offset>12246</offset><text>Prediction and classification of domain structural classes</text></passage><passage><infon key="fpage">172</infon><infon key="lpage">185</infon><infon key="name_0">surname:Bahar;given-names:I</infon><infon key="name_1">surname:Atilgan;given-names:AR</infon><infon key="name_2">surname:Jemigan;given-names:RL</infon><infon key="name_3">surname:Erman;given-names:B</infon><infon key="pub-id_doi">10.1002/(SICI)1097-0134(199710)29:2&lt;172::AID-PROT5&gt;3.0.CO;2-F</infon><infon key="pub-id_pmid">9329082</infon><infon key="section_type">REF</infon><infon key="source">Proteins</infon><infon key="type">ref</infon><infon key="volume">29</infon><infon key="year">1997</infon><offset>12305</offset><text>Understanding the recognition of protein structural classes by amino acid composition</text></passage><passage><infon key="fpage">216</infon><infon key="lpage">224</infon><infon key="name_0">surname:Chou;given-names:KC</infon><infon key="pub-id_doi">10.1006/bbrc.1999.1325</infon><infon key="pub-id_pmid">10527868</infon><infon key="section_type">REF</infon><infon key="source">Biochem Biophys Res Commun</infon><infon key="type">ref</infon><infon key="volume">264</infon><infon key="year">1999</infon><offset>12391</offset><text>A key driving force in determination of protein structural classes</text></passage><passage><infon key="fpage">275</infon><infon key="lpage">349</infon><infon key="name_0">surname:Chou;given-names:KC</infon><infon key="name_1">surname:Zhang;given-names:CT</infon><infon key="pub-id_pmid">7587280</infon><infon key="section_type">REF</infon><infon key="source">Critical Reviews in Biochemistry and Molecular Biology</infon><infon key="type">ref</infon><infon key="volume">30</infon><infon key="year">1995</infon><offset>12458</offset><text>Prediction of Protein Structural Classes</text></passage><passage><infon key="fpage">171</infon><infon key="lpage">208</infon><infon key="name_0">surname:Chou;given-names:KC</infon><infon key="pub-id_pmid">12369916</infon><infon key="section_type">REF</infon><infon key="source">Current Protein and Peptide Science</infon><infon key="type">ref</infon><infon key="volume">1</infon><infon key="year">2001</infon><offset>12499</offset><text>Review: Prediction of protein structural classes and subcellular location</text></passage><passage><infon key="name_0">surname:Vapnik;given-names:VN</infon><infon key="section_type">REF</infon><infon key="source">Springer,</infon><infon key="type">ref</infon><infon key="year">1995</infon><offset>12573</offset><text>The Nature of Statistical Learning Theory</text></passage><passage><infon key="fpage">729</infon><infon key="lpage">738</infon><infon key="name_0">surname:Zhou;given-names:GP</infon><infon key="pub-id_doi">10.1023/A:1020713915365</infon><infon key="pub-id_pmid">9988519</infon><infon key="section_type">REF</infon><infon key="source">Journal of Protein Chemistry</infon><infon key="type">ref</infon><infon key="volume">17</infon><infon key="year">1998</infon><offset>12615</offset><text>An Intriguing Controversy over Protein Structural Class Prediction</text></passage><passage><infon key="fpage">536</infon><infon key="lpage">540</infon><infon key="name_0">surname:Murzin;given-names:AG</infon><infon key="name_1">surname:Brenner;given-names:SE</infon><infon key="name_2">surname:Hubbard;given-names:T</infon><infon key="name_3">surname:Chothia;given-names:C</infon><infon key="pub-id_doi">10.1006/jmbi.1995.0159</infon><infon key="section_type">REF</infon><infon key="source">Jourani of Molecular Biology</infon><infon key="type">ref</infon><infon key="volume">247</infon><infon key="year">1995</infon><offset>12682</offset><text>SCOP: a structural classification of protein database for the investigation of sequence and structures</text></passage><passage><infon key="fpage">336</infon><infon key="lpage">338</infon><infon key="name_0">surname:Cai;given-names:YD</infon><infon key="pub-id_doi">10.1002/prot.1045.abs</infon><infon key="section_type">REF</infon><infon key="source">Proteins: Structure, Function and Genetics</infon><infon key="type">ref</infon><infon key="volume">43</infon><infon key="year">2001</infon><offset>12785</offset><text>Is it a paradox or misinterpretation?</text></passage><passage><infon key="fpage">57</infon><infon key="lpage">59</infon><infon key="name_0">surname:Zhou;given-names:GP</infon><infon key="name_1">surname:Assa-Munt;given-names:N</infon><infon key="pub-id_doi">10.1002/prot.1071</infon><infon key="section_type">REF</infon><infon key="source">Proteins: Structure, Function and Genetics</infon><infon key="type">ref</infon><infon key="volume">44</infon><infon key="year">2001</infon><offset>12823</offset><text>Some insights into protein structural class prediction</text></passage><passage><infon key="fpage">783</infon><infon key="lpage">5</infon><infon key="name_0">surname:Cai;given-names:YD</infon><infon key="name_1">surname:Zhou;given-names:GP</infon><infon key="pub-id_doi">10.1016/S0300-9084(00)01161-5</infon><infon key="pub-id_pmid">11018296</infon><infon key="section_type">REF</infon><infon key="source">Biochimie</infon><infon key="type">ref</infon><infon key="volume">82(8)</infon><infon key="year">2000</infon><offset>12878</offset><text>Prediction of protein structural classes by neural network</text></passage><passage><infon key="name_0">surname:Vapnik;given-names:VN</infon><infon key="section_type">REF</infon><infon key="source">Wiley-Interscience, New York,</infon><infon key="type">ref</infon><infon key="year">1998</infon><offset>12937</offset><text>Statistical Learning Theory</text></passage><passage><infon key="fpage">1</infon><infon key="lpage">4</infon><infon key="name_0">surname:Robert;given-names:B</infon><infon key="name_1">surname:Matthew;given-names:T</infon><infon key="name_2">surname:Sean;given-names:H</infon><infon key="name_3">surname:Bernard;given-names:B</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the AISB'00 Symposium on Artificial Intelligence in Bioinformatics</infon><infon key="type">ref</infon><infon key="year">2000</infon><offset>12965</offset><text>Drug Design by Machine Learning: Support Vector Machine for Pharmaceutical Data Analysis</text></passage><passage><infon key="name_0">surname:Joachims;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">Proceedings of the European Conference on Machine Learning, Springer,</infon><infon key="type">ref</infon><infon key="year">1998</infon><offset>13054</offset><text>Text Categorization with Support Vector Machines: Learning with Many Relevant Features&quot;</text></passage><passage><infon key="fpage">262</infon><infon key="lpage">267</infon><infon key="name_0">surname:Brown;given-names:MPS</infon><infon key="name_1">surname:Grundy;given-names:WN</infon><infon key="name_2">surname:Lin;given-names:D</infon><infon key="name_3">surname:Cristianini;given-names:N</infon><infon key="name_4">surname:Sugnet;given-names:C</infon><infon key="name_5">surname:Ares;given-names:JM</infon><infon key="name_6">surname:Haussler;given-names:D</infon><infon key="pub-id_doi">10.1073/pnas.97.1.262</infon><infon key="pub-id_pmid">10618406</infon><infon key="section_type">REF</infon><infon key="source">Proc Natl Acad Sci</infon><infon key="type">ref</infon><infon key="volume">97</infon><infon key="year">2000</infon><offset>13142</offset><text>Knowledge-based Analysis of Microarray Gene Expression Data by using Support Vector Machines</text></passage><passage><infon key="fpage">349</infon><infon key="lpage">358</infon><infon key="name_0">surname:Ding;given-names:CHQ</infon><infon key="name_1">surname:Dubchak;given-names:I</infon><infon key="pub-id_doi">10.1093/bioinformatics/17.4.349</infon><infon key="pub-id_pmid">11301304</infon><infon key="section_type">REF</infon><infon key="source">Bioinformatics</infon><infon key="type">ref</infon><infon key="volume">4(17)</infon><infon key="year">2001</infon><offset>13235</offset><text>Multi-class Protein Fold Recognition Using Support Vector Machines and Neural Networks</text></passage><passage><infon key="fpage">11</infon><infon key="name_0">surname:Joachims;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">Advances in Kernel Methods -Support Vector Learning, B Scholkopf and C Burges and A Smola (ed), MIT Press,</infon><infon key="type">ref</infon><infon key="year">1999</infon><offset>13322</offset><text>Making large-Scale SVM Learning Practical</text></passage><passage><infon key="name_0">surname:Joachims;given-names:T</infon><infon key="section_type">REF</infon><infon key="source">International Conference on Machine Learning (ICML),1996b</infon><infon key="type">ref</infon><offset>13364</offset><text>Transductive Inference for Text Classification using Support Vector Machines</text></passage><passage><infon key="fpage">273</infon><infon key="lpage">293</infon><infon key="name_0">surname:Cortes;given-names:C</infon><infon key="name_1">surname:Vapnik;given-names:VN</infon><infon key="pub-id_doi">10.1023/A:1022627411411</infon><infon key="section_type">REF</infon><infon key="source">Machine Learning</infon><infon key="type">ref</infon><infon key="volume">20</infon><infon key="year">1995</infon><offset>13441</offset><text>Support vector networks</text></passage></document></collection>
